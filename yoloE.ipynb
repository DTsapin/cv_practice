{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ultralytics import YOLOE\n",
    "from ultralytics.models.yolo.yoloe import YOLOEVPSegPredictor\n",
    "import supervision as sv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–æ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /mnt/d/my_cv_projecs/yolo/sample_substraction/240.jpg: 480x640 (no detections), 101.3ms\n",
      "Speed: 3.7ms preprocess, 101.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLOE(\"yoloe-11l-seg.pt\").cuda()\n",
    "image_size = (640, 640)\n",
    "image = 'sample_substraction/240.jpg'\n",
    "license_plate_prompt = \"\"\"\n",
    "Russian car license plate: \n",
    "- white rectangular-shape metal plate\n",
    "- black cyrillic characters\n",
    "- format: 1 letter, 3 digits, 2 letters, regional code\n",
    "- example: A 123 BC 77 RUS\n",
    "- mounted with visible bolts/screws\n",
    "\"\"\"\n",
    "classes = [license_plate_prompt]\n",
    "# –í–º–µ—Å—Ç–µ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ - –ø–µ—Ä–µ–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "model.set_classes(classes, model.get_text_pe(classes))\n",
    "results = model.predict(image, \n",
    "                        imgsz = image_size, \n",
    "                        iou = 1e-9,\n",
    "                        conf = 0.6)\n",
    "detections = sv.Detections.from_ultralytics(results[0])\n",
    "\n",
    "annotator = sv.BoxAnnotator()\n",
    "# –î–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ - –ø–∞–π–ø–ª–∞–π–Ω –ø–æ —Å—É—Ç–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–∞\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "labels = [f'{confidence:.2f}' \n",
    "          for confidence \n",
    "          in detections.confidence]\n",
    "\n",
    "image_for_labeling = cv2.imread(image)\n",
    "image_with_bounding_boxes = annotator.annotate(scene = image_for_labeling, \n",
    "                                               detections = detections)\n",
    "segmented_image_with_bounding_boxes = mask_annotator.annotate(scene = image_with_bounding_boxes,\n",
    "                                                              detections = detections)\n",
    "labeled_image = label_annotator.annotate(scene = segmented_image_with_bounding_boxes, \n",
    "                                         detections = detections,\n",
    "                                         labels = labels)\n",
    "cv2.imwrite('labeled_by_text_prompt.jpg', labeled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.161 üöÄ Python-3.12.3 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLOe-11l-seg summary (fused): 227 layers, 35,117,862 parameters, 2,254,374 gradients\n",
      "\n",
      "image 1/1 /mnt/d/my_cv_projecs/yolo/sample_substraction/240.jpg: 480x640 1 object0, 49.4ms\n",
      "Speed: 2.5ms preprocess, 49.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLOE(\"yoloe-11l-seg.pt\").cuda()\n",
    "image_size = (640, 640)\n",
    "target_image = 'sample_substraction/240.jpg'\n",
    "refer_image = 'refer_image_12.jpg'\n",
    "\n",
    "xyxy_boxes_coords = np.array([[802.66,       \n",
    "                               732.7,        \n",
    "                               1032,       \n",
    "                               793.6]])\n",
    "\n",
    "# –í–∏–∑—É–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ - —É–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ü–µ–ª–µ–≤–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –∏ –∫–ª–∞—Å—Å –æ–±—ä–µ–∫—Ç–∞\n",
    "# bboxes - –Ω—É–∂–Ω–æ –ø–æ–¥–∞–≤–∞—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ XYXY (x1, y1, x2, y2)\n",
    "# TODO –¥–µ—Ç–∞–ª—å–Ω–æ –∏–∑—É—á–∏—Ç—å, –∫–∞–∫ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, \n",
    "# –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–µ–π—Å—ã —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞\n",
    "visual_prompt = dict(\n",
    "    bboxes = xyxy_boxes_coords,\n",
    "    cls = np.array([0])\n",
    ")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞\n",
    "results = model.predict(\n",
    "    source = target_image,\n",
    "    refer_image = refer_image,\n",
    "    visual_prompts = visual_prompt,\n",
    "    imgsz = image_size,\n",
    "    iou = 1e-9,\n",
    "    conf = 0.6,\n",
    "    predictor = YOLOEVPSegPredictor\n",
    ")\n",
    "\n",
    "detections = sv.Detections.from_ultralytics(results[0])\n",
    "\n",
    "annotator = sv.BoxAnnotator()\n",
    "# –î–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ - –ø–∞–π–ø–ª–∞–π–Ω –ø–æ —Å—É—Ç–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–∞\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "labels = [f'{confidence:.2f}' \n",
    "          for confidence \n",
    "          in detections.confidence]\n",
    "\n",
    "image_for_labeling = cv2.imread(target_image)\n",
    "image_with_bounding_boxes = annotator.annotate(scene = image_for_labeling, \n",
    "                                               detections = detections)\n",
    "segmented_image_with_bounding_boxes = mask_annotator.annotate(scene = image_with_bounding_boxes,\n",
    "                                                              detections = detections)\n",
    "labeled_image = label_annotator.annotate(scene = segmented_image_with_bounding_boxes, \n",
    "                                         detections = detections,\n",
    "                                         labels = labels)\n",
    "cv2.imwrite('labeled_by_visual_prompt.jpg', labeled_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
